{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CleanedDataNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubha\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "#from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "# from keras.utils.np_utils import probas_to_classes\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from random import shuffle\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "#import word2vecReader as godin_embedding\n",
    "#from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#import skopt\n",
    "#from skopt import gp_minimize, forest_minimize\n",
    "#from skopt.space import Real, Categorical, Integer\n",
    "#from skopt.utils import use_named_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df_new= df.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Century-old mud houses amidst concrete jungle:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group of ministers formed to unlock land of cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dacoits batter infant in Garia: Seven armed da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TMC warns workers against sand mining: A day a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Policymakers should have bigger vision': A se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Century-old mud houses amidst concrete jungle:...       0\n",
       "1  Group of ministers formed to unlock land of cl...       0\n",
       "2  Dacoits batter infant in Garia: Seven armed da...       1\n",
       "3  TMC warns workers against sand mining: A day a...       0\n",
       "4  'Policymakers should have bigger vision': A se...       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "#     print(sentence)\n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "#     print(sentence)\n",
    "    #removes links\n",
    "    #sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "#     print(sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "#     print(sentence)\n",
    "    #removing stock names to see if it helps\n",
    "#     sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    sentence = sentence.replace(\"'s\",'')\n",
    "    sentence = sentence.replace(\"-\",' ')\n",
    "#     print(sentence)\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "#     print(tokens)\n",
    "    #     remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "#     for w in stop_words:\n",
    "#         print(w)\n",
    "#     print(tokens)\n",
    "    # filter out short tokens\n",
    "#     tokens = [word for word in tokens if len(word) > 1]\n",
    "#     print(tokens)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "#     print(tokens)\n",
    "    tokens = [w.translate(remove_digits) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "#     print(tokens)\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not needed here since this is binary classification \n",
    "def convert_lables(trainY,testY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY+testY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,no_of_classes),le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= df_new['text']\n",
    "labels  = df_new['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [clean_sentence(x) for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(s.split()) for s in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  20\n"
     ]
    }
   ],
   "source": [
    "print('max len = ',max(lengths))\n",
    "#sns.distplot(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deviding it into train and test\n",
    "max_length  = max(lengths)\n",
    "X_train = sentence[:4000]\n",
    "X_test = sentence[4000:]\n",
    "Y_train= labels[:4000]\n",
    "Y_test = labels[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "tokenizer = create_tokenizer(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "trainX = encode_text(tokenizer, X_train, max_length)\n",
    "# testX = encode_text(tokenizer, testX, max_length)\n",
    "#trainY,lable_encoding = convert_lables(Y,[])\n",
    "#Testing data\n",
    "tokenizer = create_tokenizer(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "testX = encode_text(tokenizer, X_test, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11313\n",
      "4000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(len(trainX))\n",
    "\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_GloVe_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9919f1b3bb6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeddings_index_glove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_GloVe_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove.6B.100d.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_GloVe_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = load_GloVe_embedding('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings = get_GloVe_embedding_matrix(embeddings_index_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each sentence\n",
    "#1. See all words's emebeddings (of size 100 for each word)\n",
    "#2. Take the mean of A[0] ,B[0], C[0].....N[0], such that A, B,C .. N are words of embeddings.\n",
    "#3. Do this till A[100]...N[100]\n",
    "#4. This creates a vector sentence1 : [mean of first bit of  words........... mean of last bit of words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the mean of all the words in the sentence\n",
    "def mean_vector_sentence(sentence):\n",
    "    l1=[]\n",
    "    for word in sentence.split():\n",
    "        l1.append(get_embeddings[tokenizer.word_index[word]])\n",
    "    mat = np.array(l1)\n",
    "    v=np.mean(mat)\n",
    "    return v\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each sentence is now a vector , taken by mean of vectors of the words in the sentence\n",
    "vect_sentence=[]\n",
    "for s in sentence:\n",
    "    vect_sentence.append(mean_vector_sentence(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_sentence)\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for example title for each title will be created such.\n",
    "#Will content embeddings be done sentence wise or the entire row content into  one vector\n",
    "#These embeddings will be further used in the cnn network for the fused main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "filter_sizes =4\n",
    "num_filters = 512\n",
    "drop = 0.4\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PImage\n",
    "from os import listdir\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.applications.vgg19 import VGG19\n",
    "#function to find the image vector fro pre-trained VGG19 Model\n",
    "#Datasets of fake and real images for each article\n",
    "#file being the name of the folder of images.\n",
    "def image_embeddings(file):\n",
    "    imagesList = listdir(file)\n",
    "    \n",
    "\n",
    "\n",
    "    model= VGG19(weights='imagenet')\n",
    "    \n",
    "    final_image = []\n",
    "    for img in imagesList:\n",
    "        img= file+'/'+img\n",
    "        #img = PImage.open(file +'/'+ img)\n",
    "        image = load_img(img, target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "# predict the probability across all output classes\n",
    "        yhat = model.predict(image)\n",
    "        final_image.append(yhat)\n",
    "# convert the probabilities to class labels\n",
    "        label = decode_predictions(yhat)\n",
    "        \n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "        label = label[0][0]\n",
    "# print the classification\n",
    "       # print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "    return final_image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length_title, vocab_size_title, length_content , vocab_size_content,rel_embeddings):\n",
    "\t# channel 1 : title\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size,100)(inputs1)\n",
    "    #Filter size:4 \n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "    \n",
    "\t# channel 2 :  CONTENT + KG\n",
    "\tinputs2 = Input(shape=(length_content,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n",
    "\t#drop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t#should this be embedding content kalength?\n",
    "\tinputs3 = Input(shape=(length_content,))\n",
    "\t#embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(rel_embedding)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "    \n",
    "    merged_content = concatenate([flat2,flat3])\n",
    "   \n",
    "    #IMAGES\n",
    "    inputs4 = #max length of  of image embedding\n",
    "    image_embedding = image_embeddings('fake')\n",
    "    \n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, merged_content ,image_embeddings])\n",
    "    #merged2= conactenate(image_embeddings)\n",
    "\t# interpretation\n",
    "    \n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "    #dense2 = Dense(10,activation='relu')(merged2)\n",
    "\toutputs = Dense(1, activation='softmax')(dense1)\n",
    "    #what should be put in input for  image embedding?\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3, inputs4], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='final.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for title\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1 : title\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "    #embedding dimention :256\n",
    "\tembedding1 = Embedding(vocab_size,100)(inputs1)\n",
    "    #Filter size:4 \n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    #This is what I think it should be,becuase embedding dimention is256\n",
    "    #conv2= Conv2D(num_filters, kernel_size=(4,256), padding='valid', kernel_initializer='normal', activation='relu')(embedding1)\n",
    "\t#drop1 = Dropout(0.5)(conv1)\n",
    "    #pool1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv2)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\tmerged = flat1\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=inputs1, outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tprint(model.summary())\n",
    "\treturn model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 100)           1131300   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 17, 32)            12832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,146,713\n",
      "Trainable params: 1,146,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model =define_model(max_length,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 4s 892us/step - loss: 0.3593 - acc: 0.9170\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 2s 432us/step - loss: 0.1784 - acc: 0.9245\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 2s 435us/step - loss: 0.0749 - acc: 0.9245\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 2s 440us/step - loss: 0.0561 - acc: 0.9708\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 2s 430us/step - loss: 0.0496 - acc: 0.9988\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 2s 432us/step - loss: 0.0215 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 2s 442us/step - loss: 0.0046 - acc: 0.9995\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 2s 432us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 2s 452us/step - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 2s 424us/step - loss: 3.8362e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history_object = model.fit(trainX,Y_train,epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.15608265e-04],\n",
       "       [1.26477360e-04],\n",
       "       [8.09503399e-06],\n",
       "       ...,\n",
       "       [1.51686225e-04],\n",
       "       [5.76546190e-05],\n",
       "       [3.76895279e-08]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p #Will this go into the model in final predict? #Ask Simra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2326"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2326/2326 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5685145368780348, 0.9333619946871763]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
