{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "#from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "# from keras.utils.np_utils import probas_to_classes\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from random import shuffle\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "#import word2vecReader as godin_embedding\n",
    "#from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#import skopt\n",
    "#from skopt import gp_minimize, forest_minimize\n",
    "#from skopt.space import Real, Categorical, Integer\n",
    "#from skopt.utils import use_named_argsfrom keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "#     print(sentence)\n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "#     print(sentence)\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "#     print(tokens)\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "#     for w in stop_words:\n",
    "#         print(w)\n",
    "#     print(tokens)\n",
    "    # filter out short tokens\n",
    "#     tokens = [word for word in tokens if len(word) > 1]\n",
    "#     print(tokens)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "#     print(tokens)\n",
    "    tokens = [w.translate(remove_digits) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "#     print(tokens)\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index,vocab_size,tokenizer):\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title and Content\n",
    "\n",
    "def work(sentence,X_train,embeddings_index_glove):#,X_test):\n",
    "    sentence = [clean_sentence(x) for x in sentence]\n",
    "    lengths = [len(s.split()) for s in sentence]\n",
    "    print('max len = ',max(lengths))\n",
    "    max_length  = max(lengths)\n",
    "    \n",
    "    #Training data\n",
    "    tokenizer = create_tokenizer(X_train)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    trainX = encode_text(tokenizer, X_train, max_length)\n",
    "\n",
    "    #Testing data\n",
    "    #tokenizer = create_tokenizer(X_test)\n",
    "    #vocab_size = len(tokenizer.word_index) + 1\n",
    "    #testX = encode_text(tokenizer, X_test, max_length)\n",
    "    \n",
    "\n",
    "    get_embeddings = get_GloVe_embedding_matrix(embeddings_index_glove,vocab_size,tokenizer)\n",
    "    \n",
    "    return tokenizer,max_length,vocab_size,get_embeddings\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = load_GloVe_embedding('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "df = pd.read_csv('cleaned_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding sentence and content\n",
    "sentence_title= df['Title']\n",
    "sentence_content  = df['Content']\n",
    "label= df['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining training and testing\n",
    "X_train_title = sentence_title\n",
    "Y_train_title = label\n",
    "X_train_content = sentence_content\n",
    "Y_train_content = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  64\n",
      "max len =  6435\n"
     ]
    }
   ],
   "source": [
    "tokenizer_title,length_title,vocab_size_title,get_embeddings_title = work(sentence_title,X_train_title,embeddings_index_glove)\n",
    "#similarly for content\n",
    "tokenizer_content,length_content,vocab_size_content,get_embeddings_content = work(sentence_content,X_train_content,embeddings_index_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Relation data\n"
     ]
    }
   ],
   "source": [
    "#Loading KG Embedding\n",
    "print('Loading Relation data')\n",
    "X_fake = np.load('SpotFake-master/KG_ours/final_fake_entRel.npy',encoding='latin1')\n",
    "X_real = np.load('SpotFake-master/KG_ours/final_real_entRel.npy',encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_embeddings=np.append(X_fake,X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying VGG Model to Images\n",
    "from PIL import Image as PImage\n",
    "from os import listdir\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.applications.vgg19 import decode_predictions\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import optimizers\n",
    "#function to find the image vector fro pre-trained VGG19 Model\n",
    "#Datasets of fake and real images for each article\n",
    "#file being the name of the folder of images.\n",
    "def image_embeddings(file):\n",
    "    imagesList = listdir(file)\n",
    "    model= VGG19(weights='imagenet')\n",
    "    #model_ = Sequential()\n",
    "    #for layer in model.layers[:-1]:  #-1 to get rid of last layer\n",
    "       # model_.add(layer)\n",
    "    #model_.summary() \n",
    "    \n",
    "    final_image = []\n",
    "    for img in imagesList:\n",
    "        img= file+'/'+img\n",
    "        #img = PImage.open(file +'/'+ img)\n",
    "        image = load_img(img, target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        yhat = model.predict(image)\n",
    "# predict the probability across all output classes\n",
    "        final_image.append(yhat)\n",
    "# convert the probabilities to class labels\n",
    "       \n",
    "    return final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anubha\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:918: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n",
      "C:\\Users\\Anubha\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:918: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    }
   ],
   "source": [
    "#Training on existing images\n",
    "fake_pic= image_embeddings('pics/fake_pics')\n",
    "real_pic =image_embeddings('pics/real_pics')\n",
    "final_pict = fake_pic+real_pic\n",
    "df['Image'] = final_pict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-64-7954f2c1a20e>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-64-7954f2c1a20e>\"\u001b[1;36m, line \u001b[1;32m46\u001b[0m\n\u001b[1;33m    merged_content = concatenate([flat2,mergedke])\u001b[0m\n\u001b[1;37m                                                  ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "def define_model(length_title, vocab_size_title, length_content , vocab_size_content,rel_embeddings,get_embeddings_title,get_embeddings_content,final_pict):\n",
    "    #=========================================================================\n",
    "\t# channel 1 : title\n",
    "\tinputs1 = Input(shape=(length_title,))\n",
    "\tembedding1 = Embedding(vocab_size_title,100)(inputs1)\n",
    "    #Filter size:4 \n",
    "\tconv1 = Conv1D(filters=32, weights=[get_embeddings_title],kernel_size=4, activation='relu')(embedding1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "    #----------------------------------------------------------\n",
    "\t# channel 2 :  CONTENT + KG\n",
    "\tinputs2 = Input(shape=(length_content,))\n",
    "\tembedding2 = Embedding(vocab_size_content, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32,weights=[get_embeddings_content], kernel_size=4, activation='relu')(embedding2)\n",
    "\t#drop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t#should this be embedding content ka length?\n",
    "\n",
    "\tinputs1k = Input(shape=(length_content,))\n",
    "\tembedding1k = Embedding(vocab_size_content, 100)(inputs1k)\n",
    "\tconv1k = Conv1D(filters=32, kernel_size=3, weights=[rel_embeddings],activation='relu')(embedding1k)\n",
    "\t#drop1 = Dropout(0.5)(conv1)\n",
    "\tpool1k = MaxPooling1D(pool_size=2)(onv1k)\n",
    "\tflat1k = Flatten()(pool1k)\n",
    "\t# channel 2\n",
    "\tinputs2k = Input(shape=(length_content,))\n",
    "\tembedding2 = Embedding(vocab_size_content, 100)(inputs2)\n",
    "\tconv2k = Conv1D(filters=32, weights=[rel_embeddings],kernel_size=4, activation='relu')(embedding2k)\n",
    "\t#drop2 = Dropout(0.5)(conv2)\n",
    "\tpool2k = MaxPooling1D(pool_size=2)(conv2k)\n",
    "\tflat2k = Flatten()(pool2k)\n",
    "\t# channel 3\n",
    "\tinputs3k = Input(shape=(length_content,))\n",
    "\tembedding3k = Embedding(vocab_size_content, 100)(inputs3k)\n",
    "\tconv3k = Conv1D(filters=32, kernel_size=5, weights=[rel_embeddings],activation='relu')(embedding3k)\n",
    "\t#drop3 = Dropout(0.5)(conv3)\n",
    "\tpool3k = MaxPooling1D(pool_size=2)(conv3k)\n",
    "\tflat3k = Flatten()(pool3k)\n",
    "\t# merge\n",
    "\tmergedke = concatenate([flat1k, flat2k, flat3k])\n",
    "    \n",
    "    merged_content = concatenate([flat2,mergedke])\n",
    "    #-------------------------------------------------------------\n",
    "   \n",
    "    #IMAGES\n",
    "    inputs4 = Input(shape=len(image_embedding[0]),)#max length of  of image embedding\n",
    "    \n",
    "    image_embedding =final_pict\n",
    "\n",
    "   #======================================================= \n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, merged_content ,image_embedding])\n",
    "    #merged2= conactenate(image_embeddings)\n",
    "\t# interpretation\n",
    "    \n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "    #dense2 = Dense(10,activation='relu')(merged2)\n",
    "\toutputs = Dense(1, activation='softmax')(dense1)\n",
    "    #what should be put in input for  image embedding?\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs4], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='final.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
