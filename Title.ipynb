{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CleanedDataNew.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation,digits\n",
    "import re\n",
    "from keras.utils import to_categorical\n",
    "#from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "# from keras.utils.np_utils import probas_to_classes\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from random import shuffle\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 11.7,8.27\n",
    "#import word2vecReader as godin_embedding\n",
    "#from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#import skopt\n",
    "#from skopt import gp_minimize, forest_minimize\n",
    "#from skopt.space import Real, Categorical, Integer\n",
    "#from skopt.utils import use_named_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df_new= df.drop(columns=['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Century-old mud houses amidst concrete jungle:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Group of ministers formed to unlock land of cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dacoits batter infant in Garia: Seven armed da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TMC warns workers against sand mining: A day a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Policymakers should have bigger vision': A se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Century-old mud houses amidst concrete jungle:...       0\n",
       "1  Group of ministers formed to unlock land of cl...       0\n",
       "2  Dacoits batter infant in Garia: Seven armed da...       1\n",
       "3  TMC warns workers against sand mining: A day a...       0\n",
       "4  'Policymakers should have bigger vision': A se...       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,' ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    #remove multiple repeat non num-aplha char !!!!!!!!!-->!\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "#     print(sentence)\n",
    "    #removes alpha char repeating more than twice aaaa->aa\n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "#     print(sentence)\n",
    "    #removes links\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', r'', sentence)\n",
    "#     print(sentence)\n",
    "    # remove @usernames\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", \"\", sentence)\n",
    "#     print(sentence)\n",
    "    #removing stock names to see if it helps\n",
    "#     sentence = re.sub(r\"(?:\\$|https?\\://)\\S+\", \"\", sentence)\n",
    "    #remove # from #tags\n",
    "    sentence = sentence.replace('#','')\n",
    "    sentence = sentence.replace(\"'s\",'')\n",
    "    sentence = sentence.replace(\"-\",' ')\n",
    "#     print(sentence)\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    tokens = [remove_punctuation(w) for w in tokens]\n",
    "#     print(tokens)\n",
    "    #     remove remaining tokens that are not alphabetic\n",
    "#     tokens = [word for word in tokens if word.isalpha()]\n",
    "#no removing non alpha words to keep stock names($ZSL)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "#     for w in stop_words:\n",
    "#         print(w)\n",
    "#     print(tokens)\n",
    "    # filter out short tokens\n",
    "#     tokens = [word for word in tokens if len(word) > 1]\n",
    "#     print(tokens)\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "#     print(tokens)\n",
    "    tokens = [w.translate(remove_digits) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "#     print(tokens)\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not needed here since this is binary classification \n",
    "def convert_lables(trainY,testY):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(trainY+testY)\n",
    "    temp1 = le.transform(trainY)\n",
    "    return to_categorical(temp1,no_of_classes),le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading GloVe embedding\n",
    "def load_GloVe_embedding(file_name):\n",
    "    embeddings_index = dict()\n",
    "    f = open(file_name,encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "def get_GloVe_embedding_matrix(embeddings_index):\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= df_new['text']\n",
    "labels  = df_new['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [clean_sentence(x) for x in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(s.split()) for s in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  20\n"
     ]
    }
   ],
   "source": [
    "print('max len = ',max(lengths))\n",
    "#sns.distplot(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length  = max(lengths)\n",
    "X = sentence\n",
    "Y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "trainX = encode_text(tokenizer, X, max_length)\n",
    "# testX = encode_text(tokenizer, testX, max_length)\n",
    "#trainY,lable_encoding = convert_lables(Y,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15207\n",
      "6326\n",
      "6326\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(len(trainX))\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index_glove = load_GloVe_embedding('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings = get_GloVe_embedding_matrix(embeddings_index_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.018118  ,  0.28816   ,  0.52890003,  0.64172   ,  0.14843   ,\n",
       "        0.081948  , -0.65104002,  0.83174002,  0.17551   , -0.53397   ,\n",
       "       -0.26334   ,  0.17964   , -0.12782   ,  0.06143   , -0.46009001,\n",
       "       -0.68702   ,  0.2938    , -0.29495999,  0.45078   ,  1.12720001,\n",
       "       -0.10113   ,  0.31600001, -0.97586   , -0.21569   ,  0.034138  ,\n",
       "       -0.37158999, -0.85093999, -0.49879   ,  0.020042  , -1.39919996,\n",
       "       -0.061732  ,  0.96856999,  0.055472  ,  0.042682  ,  0.051694  ,\n",
       "        1.06620002, -0.17106   , -0.35253999, -0.43452999, -0.12687001,\n",
       "        0.75879002,  0.46337   , -0.16287   , -0.37125999, -0.45442   ,\n",
       "       -0.24525   , -0.96293002, -0.32804999, -0.025023  , -0.013757  ,\n",
       "       -0.067559  , -0.31332001, -0.14564   ,  0.47632   , -0.82901001,\n",
       "       -0.78926003,  0.25163999, -0.66337001,  1.13100004, -0.24765   ,\n",
       "        1.23829997, -0.65959001, -0.028934  , -0.85433   ,  0.21554001,\n",
       "        0.002759  ,  0.24817   ,  0.11609   ,  0.40447   ,  0.48199001,\n",
       "        0.20746   , -0.51564997, -0.086789  ,  0.58723003,  0.85570002,\n",
       "       -0.17826   ,  0.40450001, -0.41723999, -0.49797001,  0.22044   ,\n",
       "        0.80123001,  0.50551999, -0.34033   , -0.85628003, -0.80782998,\n",
       "       -0.036799  , -0.88964999, -0.11315   ,  0.39416999, -0.034693  ,\n",
       "        0.010918  , -0.16320001,  0.26688999, -0.43461001, -0.4756    ,\n",
       "        0.29174   ,  0.47481999, -0.15511   ,  0.29761001, -0.12769   ])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings for each word existing in the given dataset\n",
    "get_embeddings[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each sentence\n",
    "#1. See all words's emebeddings (of size 100 for each word)\n",
    "#2. Take the mean of A[0] ,B[0], C[0].....N[0], such that A, B,C .. N are words of embeddings.\n",
    "#3. Do this till A[100]...N[100]\n",
    "#4. This creates a vector sentence1 : [mean of first bit of  words........... mean of last bit of words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the mean of all the words in the sentence\n",
    "def mean_vector_sentence(sentence):\n",
    "    l1=[]\n",
    "    for word in sentence.split():\n",
    "        l1.append(get_embeddings[tokenizer.word_index[word]])\n",
    "    mat = np.array(l1)\n",
    "    v=np.mean(mat)\n",
    "    return v\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each sentence is now a vector , taken by mean of vectors of the words in the sentence\n",
    "vect_sentence=[]\n",
    "for s in sentence:\n",
    "    vect_sentence.append(mean_vector_sentence(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6326"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect_sentence)\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for example title for each title will be created such.\n",
    "#Will content embeddings be done sentence wise or the entire row content into  one vector\n",
    "#These embeddings will be further used in the cnn network for the fused main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "filter_sizes =4\n",
    "num_filters = 512\n",
    "drop = 0.4\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-2-377eace80196>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-377eace80196>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    conv_Title = Conv1D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape1)\u001b[0m\n\u001b[1;37m                                                                                                                                                             ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv_Title = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "# One more  layer for content(1)\n",
    "# Fuse content and knowledge graph layer(2)\n",
    "# One more layer for image(3)\n",
    "# Fuse (1) (2) (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1 : title\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "    #embedding dimention :256\n",
    "\tembedding1 = Embedding(vocab_size,100)(inputs1)\n",
    "    #Filter size:4 \n",
    "\t#conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    #This is what I think it should be,becuase embedding dimention is256\n",
    "    conv2= Conv2D(num_filters, kernel_size=(4,256), padding='valid', kernel_initializer='normal', activation='relu')(embedding1)\n",
    "\t#drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv2)\n",
    "\t#pool1 = MaxPooling2D(pool_size=2)(conv2)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "    \n",
    "\t# channel 2 :  CONTENT + KG\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\t#drop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3 #IMAGES\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='final.png')\n",
    "\treturn model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
